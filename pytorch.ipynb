{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "training = (\n",
    "    \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "    \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\"\n",
    ")\n",
    "\n",
    "testing = (\n",
    "    \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "    \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n",
    ")\n",
    "\n",
    "def dl_mnist(ds, prefix):\n",
    "    img_url, lab_url = ds\n",
    "    print(f\"Downloading {prefix}\")\n",
    "    urllib.request.urlretrieve(img_url, f'./{prefix}-img')\n",
    "    urllib.request.urlretrieve(lab_url, f'./{prefix}-lab')\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_mnist(training, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_mnist(testing, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import gzip\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, imgfile, labelfile, transform=None):\n",
    "        self.imgfile = imgfile\n",
    "        self.labelfile = labelfile\n",
    "        self.images, self.labels = self.read_dataset(imgfile, labelfile)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = {'image': self.images[idx], 'label': self.labels[idx]}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def read_dataset(self, imgname, labname):\n",
    "        import struct\n",
    "        import gzip\n",
    "        X = []\n",
    "        y = []\n",
    "        with gzip.open(imgname, \"rb\") as img, gzip.open(labname, \"rb\") as labs:\n",
    "            img_header = struct.unpack(\">4i\", img.read(16))\n",
    "            lab_header = struct.unpack(\">2i\", labs.read(8))\n",
    "\n",
    "            img_size = img_header[2] * img_header[3]\n",
    "\n",
    "            for i in range(img_header[1]):\n",
    "                image = struct.unpack(f\"{img_size}B\", img.read(img_size))\n",
    "                label = struct.unpack(\"B\", labs.read(1))\n",
    "                image = np.array(image).reshape((28, 28))\n",
    "                X.append([image])\n",
    "                y.append(label[0])\n",
    "            X = np.array(X, dtype=\"float32\")\n",
    "        return X, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class negative(object):\n",
    "    \"\"\"Rescale values between 0 and 1\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        \n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'label': label}\n",
    "    \n",
    "compose = transforms.Compose([ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MnistDataset(\"train-img\", \"train-lab\", transform=compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MnistDataset(\"test-img\", 'test-lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test, shuffle=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_dataloader).next()\n",
    "images, labels = dataiter['image'][:10], dataiter['label'][:10]\n",
    "\n",
    "img_grid = utils.make_grid(images, nrow=5)\n",
    "# write to tensorboard\n",
    "writer.add_image('mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_dataloader):\n",
    "    images, labels = data.values()\n",
    "    break\n",
    "    \n",
    "# get the class labels for each image\n",
    "class_labels = labels\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "\n",
    "writer.add_embedding(features,\n",
    "                     metadata=class_labels,\n",
    "                     label_img=images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data(x, y):\n",
    "    img = x.numpy()[0]\n",
    "    plt.imshow(img, cmap='Greys')\n",
    "    plt.xlabel(f\"Label: {y.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data(images[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            images, labels = data['image'], data['label']\n",
    "            out = net(images)\n",
    "            out = torch.argmax(out, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += (out == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "writer = SummaryWriter('runs/MLP')\n",
    "running_loss = 0\n",
    "for e in range(epoch):\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        images, labels = data['image'], data['label']\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = net(images)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:\n",
    "            writer.add_scalar(\"accuracy\", accuracy(test_dataloader))\n",
    "            writer.add_scalar(\"loss\", running_loss / 50)\n",
    "            running_loss = 0\n",
    "    print(\"Epoch:\", e + 1, \"/\", epoch)\n",
    "print(\"Training complete\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "writer = SummaryWriter('runs/CNN')\n",
    "running_loss = 0.0\n",
    "for e in range(epoch):\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        images, labels = data['image'], data['label']\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = net(images)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:\n",
    "            writer.add_scalar(\"accuracy\", accuracy(test_dataloader))\n",
    "            writer.add_scalar(\"loss\", running_loss / 50)\n",
    "            running_loss = 0\n",
    "    print(\"Epoch:\", e + 1, \"/\", epoch)\n",
    "print(\"Training complete\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
